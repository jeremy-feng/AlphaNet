{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成卷积操作时需要的两列数据的组合的列表\n",
    "def generate_combination(N):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        N: int, the number of rows of the matrix\n",
    "\n",
    "    return:\n",
    "        combination: list, the combination of two columns of the matrix\n",
    "        combination_rev: list, the combination of two rows of the matrix, which is the reverse of combination\n",
    "    \"\"\"\n",
    "    col = []\n",
    "    col_rev = []\n",
    "    for i in range(1,N):\n",
    "        for j in range(0,i):\n",
    "            col.append([i,j])\n",
    "            col_rev.append([j,i])\n",
    "    return col, col_rev\n",
    "# 生成卷积操作时需要的两列数据的组合的列表\n",
    "combination, combination_rev = generate_combination(9)\n",
    "\n",
    "# 根据输入的矩阵和卷积操作的步长, 计算卷积操作的索引\n",
    "def get_index_list(matrix, stride):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        matrix: torch.tensor, the input matrix\n",
    "        stride: int, the stride of the convolution operation\n",
    "    \n",
    "    return:\n",
    "        index_list: list, the index of the convolution operation\n",
    "\n",
    "    \"\"\"\n",
    "    W = matrix.shape[3]\n",
    "    if W % stride == 0:\n",
    "        index_list = list(np.arange(0, W+stride, stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        index_list = list(np.arange(0, W+stride-mod, stride)) + [W]\n",
    "    return index_list\n",
    "# 根据输入的矩阵和卷积操作的步长, 计算卷积操作的索引\n",
    "# Inception模块使用的卷积操作的步长为10\n",
    "index_list = get_index_list(np.zeros((1,1,9,30)), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception, 用于提取时间序列的特征, 具体操作包括：\n",
    "\n",
    "    1. kernel_size和stride均为d=10的特征提取层, 类似于卷积层，用于提取时间序列的特征. 具体包括:\n",
    "\n",
    "        1. ts_corr4d: 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的相关系数\n",
    "        2. ts_cov4d: 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的协方差\n",
    "        3. ts_stddev4d: 过去 d 天 X 值构成的时序数列的标准差\n",
    "        4. ts_zscore4d: 过去 d 天 X 值构成的时序数列的平均值除以标准差\n",
    "        5. ts_return4d: (X - delay(X, d))/delay(X, d)-1, 其中delay(X, d)为 X 在 d 天前的取值\n",
    "        6. ts_decaylinear4d: 过去 d 天 X 值构成的时序数列的加权平均值，权数为 d, d – 1, …, 1(权数之和应为 1，需进行归一化处理)，其中离现在越近的日子权数越大\n",
    "        7. ts_mean4d: 过去 d 天 X 值构成的时序数列的平均值\n",
    "\n",
    "        各操作得到的张量维数：\n",
    "        1. 由于涉及两个变量的协方差, 因此ts_corr4d和ts_cov4d的输出为 N*1*36*3\n",
    "        2. 其余操作均只涉及单变量的时序计算, 因此输出为 N*1*9*3\n",
    "\n",
    "    2. 对第1步的输出进行Batch Normalization操作, 输出维数仍为 N*1*36*3 或 N*1*9*3\n",
    "\n",
    "    3. 对于第2步得到的张量, kernel_size为3的池化层. 具体包括:\n",
    "        1. max_pool: 过去 d 天 X 值构成的时序数列的最大值\n",
    "        2. avg_pool: 过去 d 天 X 值构成的时序数列的平均值\n",
    "        3. min_pool: 过去 d 天 X 值构成的时序数列的最小值\n",
    "\n",
    "        以上三个操作的输出均为 N*1*117*1\n",
    "    \n",
    "    4. 对第3步的输出进行Batch Normalization操作, 输出维数仍为 N*1*117*1\n",
    "\n",
    "    5. 将第2步和第4步的输出展平后进行拼接, 得到的张量维数为 N*(2*36*3+5*9*3+3*117) = N*702\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, combination, combination_rev, index_list):\n",
    "        \"\"\"\n",
    "        combination: 卷积操作时需要的两列数据的组合\n",
    "        combination_rev: 卷积操作时需要的两列数据的组合, 与combination相反\n",
    "        index_list: 卷积操作时需要的时间索引\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Inception, self).__init__()\n",
    "        # 卷积操作时需要的两列数据的组合\n",
    "        self.combination = combination\n",
    "        self.combination_rev = combination_rev\n",
    "\n",
    "        # 卷积操作时需要的时间索引\n",
    "        self.index_list = index_list\n",
    "        self.d = len(index_list)-1\n",
    "\n",
    "        # 卷积操作后的Batch Normalization层\n",
    "        self.bc1 = nn.BatchNorm2d(1)\n",
    "        self.bc2 = nn.BatchNorm2d(1)\n",
    "        self.bc3 = nn.BatchNorm2d(1)\n",
    "        self.bc4 = nn.BatchNorm2d(1)\n",
    "        self.bc5 = nn.BatchNorm2d(1)\n",
    "        self.bc6 = nn.BatchNorm2d(1)\n",
    "        self.bc7 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # 池化层, 尺度为1*d\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(1, self.d))\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=(1, self.d))\n",
    "        # 最小池化等价于相反数的最大池化, 后续会对结果取反\n",
    "        self.min_pool = nn.MaxPool2d(kernel_size=(1, self.d))\n",
    "\n",
    "        # 池化操作后的Batch Normalization层\n",
    "        self.bc_pool1 = nn.BatchNorm2d(1)\n",
    "        self.bc_pool2 = nn.BatchNorm2d(1)\n",
    "        self.bc_pool3 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        data: 输入的数据, 维度为batch_size*1*9*30\n",
    "\n",
    "        \"\"\"\n",
    "        # 本层的输入为batch_size*1*9*30, 在训练时不需要反向传播, 因此可以使用detach()函数\n",
    "        data = data.detach().cpu().numpy()\n",
    "        combination = self.combination\n",
    "        combination_rev = self.combination_rev\n",
    "\n",
    "        # 卷积操作\n",
    "        conv1 = self.ts_corr4d(data, combination, combination_rev).to(torch.float)\n",
    "        conv2 = self.ts_cov4d(data, combination, combination_rev).to(torch.float)\n",
    "        conv3 = self.ts_stddev4d(data).to(torch.float)\n",
    "        conv4 = self.ts_zcore4d(data).to(torch.float)\n",
    "        conv5 = self.ts_return4d(data).to(torch.float)\n",
    "        conv6 = self.ts_decaylinear4d(data).to(torch.float)\n",
    "        conv7 = self.ts_mean4d(data).to(torch.float)\n",
    "\n",
    "        # 卷积操作后的Batch Normalization\n",
    "        batch1 = self.bc1(conv1)\n",
    "        batch2 = self.bc2(conv2)\n",
    "        batch3 = self.bc3(conv3)\n",
    "        batch4 = self.bc4(conv4)\n",
    "        batch5 = self.bc5(conv5)\n",
    "        batch6 = self.bc6(conv6)\n",
    "        batch7 = self.bc7(conv7)\n",
    "\n",
    "        # 在 H 维度上进行特征拼接\n",
    "        feature = torch.cat(\n",
    "            [batch1, batch2, batch3, batch4, batch5, batch6, batch7], axis=2)  # N*1*(2*36+5*9)*3 = N*1*117*3\n",
    "\n",
    "        # 同时将特征展平, 准备输入到全连接层\n",
    "        feature_flatten = feature.flatten(start_dim=1)  # N*(117*3) = N*351\n",
    "\n",
    "        # 对多通道特征进行池化操作, 每层池化后面都有Batch Normalization\n",
    "        # 最大池化\n",
    "        maxpool = self.max_pool(feature)  # N*1*117*1\n",
    "        maxpool = self.bc_pool1(maxpool)\n",
    "        # 平均池化\n",
    "        avgpool = self.avg_pool(feature)  # N*1*117*1\n",
    "        avgpool = self.bc_pool2(avgpool)\n",
    "        # 最小池化\n",
    "        # N*1*117*1, 最小池化等价于相反数的最大池化, 并对结果取反\n",
    "        minpool = -self.min_pool(-1*feature)\n",
    "        minpool = self.bc_pool3(minpool)\n",
    "        # 特征拼接\n",
    "        pool_cat = torch.cat([maxpool, avgpool, minpool],\n",
    "                             axis=2)  # N*1*(3*117)*1 = N*1*351*1\n",
    "        # 将池化层的特征展平\n",
    "        pool_cat_flatten = pool_cat.flatten(start_dim=1)  # N*351\n",
    "\n",
    "        # 拼接展平后的特征\n",
    "        feature_final = torch.cat(\n",
    "            [feature_flatten, pool_cat_flatten], axis=1)  # N*(351+351) = N*702\n",
    "        return feature_final\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的相关系数\n",
    "    def ts_corr4d(self, Matrix, combination, combination_rev):\n",
    "        new_H = len(combination)\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的相关系数\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, combination, start_index:end_index]  # N*1*new_H*2*d\n",
    "            data2 = Matrix[:, :, combination_rev,\n",
    "                           start_index:end_index]  # N*1*new_H*2*d\n",
    "            std1 = data.std(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求标准差\n",
    "            std2 = data2.std(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求标准差\n",
    "            std = (std1*std2).mean(axis=3, keepdims=True)  # N*1*new_H*1*1\n",
    "            list.append(std)\n",
    "        std = np.squeeze(np.array(list)).transpose(1, 2, 0).reshape(-1, 1, new_H,\n",
    "                                                                    len(index_list)-1)+0.01  # N*1*new_H*len(index_list)-1 # 加上0.01, 防止除0\n",
    "        # N*1*new_H*len(index_list)-1\n",
    "        cov = self.ts_cov4d(Matrix, combination, combination_rev)\n",
    "        corr = cov/std  # N*1*new_H*len(index_list)-1\n",
    "        return corr\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的协方差\n",
    "    def ts_cov4d(self, Matrix, combination, combination_rev):\n",
    "        new_H = len(combination)\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的协方差\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, combination, start_index:end_index]  # N*1*new_H*2*d\n",
    "            data2 = Matrix[:, :, combination_rev,\n",
    "                           start_index:end_index]  # N*1*new_H*2*d\n",
    "            mean1 = data.mean(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求均值\n",
    "            mean2 = data2.mean(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求均值\n",
    "            spread1 = data - mean1  # N*1*new_H*2*d, 在时序上求偏差\n",
    "            spread2 = data2 - mean2  # N*1*new_H*2*d, 在时序上求偏差\n",
    "            cov = ((spread1 * spread2).sum(axis=4, keepdims=True) /\n",
    "                   (data.shape[4]-1)).mean(axis=3, keepdims=True)  # N*1*new_H*1*1\n",
    "            list.append(cov)\n",
    "        cov = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(cov)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的标准差\n",
    "    def ts_stddev4d(self, Matrix):\n",
    "        # 只需要对单变量做卷积操作, 不需要将变量两两组合。因此输出的 H 可以保持和输入的 H 一致\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的标准差\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            std = data.std(axis=3, keepdims=True)  # N*1*H*1\n",
    "            list.append(std)\n",
    "        std4d = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(std4d)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的平均值除以标准差\n",
    "    def ts_zcore4d(self, Matrix):\n",
    "        # 只需要对单变量做卷积操作, 不需要将变量两两组合。因此输出的 H 可以保持和输入的 H 一致\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的zcore\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            mean = data.mean(axis=3, keepdims=True)  # N*1*H*1\n",
    "            std = data.std(axis=3, keepdims=True) + \\\n",
    "                0.01  # N*1*H*1, 加上0.01, 防止除以0\n",
    "            list.append(mean/std)\n",
    "        zscore = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(zscore)\n",
    "\n",
    "    # (X - delay(X, d))/delay(X, d)-1, 其中 delay(X, d)为 X 在 d 天前的取值\n",
    "    def ts_return4d(self, Matrix):\n",
    "        # 只需要对单变量做卷积操作, 不需要将变量两两组合。因此输出的 H 可以保持和输入的 H 一致\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的return\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            # N*1*H*1, 在分母加上0.01, 防止除以0\n",
    "            return_ = data[:, :, :, -1]/(data[:, :, :, 0]+0.01)-1\n",
    "            list.append(return_)\n",
    "        ts_return = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(ts_return)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的加权平均值, 权数为 d, d – 1, …, 1(权数之和应为 1, 需进行归一化处理), 其中离现在越近的日子权数越大\n",
    "    def ts_decaylinear4d(self, Matrix):\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的加权平均值\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            range_ = end_index-start_index\n",
    "            weight = np.arange(1, range_+1)\n",
    "            weight = weight/weight.sum()  # 权重向量\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            wd = (data*weight).sum(axis=3, keepdims=True)  # N*1*H*1\n",
    "            list.append(wd)\n",
    "        ts_decaylinear = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(ts_decaylinear)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的平均值\n",
    "    def ts_mean4d(self, Matrix):\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的平均值\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            mean_ = data.mean(axis=3, keepdims=True)  # N*1*H*1\n",
    "            list.append(mean_)\n",
    "        ts_mean = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(ts_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaNet(nn.Module):\n",
    "\n",
    "    def __init__(self, combination, combination_rev, index_list, fc1_num, fc2_num, dropout_rate):\n",
    "        super(AlphaNet, self).__init__()\n",
    "        self.combination = combination\n",
    "        self.combination_rev = combination_rev\n",
    "        self.fc1_num = fc1_num\n",
    "        self.fc2_num = fc2_num\n",
    "        # 自定义的Inception模块\n",
    "        self.Inception = Inception(combination, combination_rev, index_list)\n",
    "        # 两个全连接层\n",
    "        self.fc1 = nn.Linear(fc1_num, fc2_num) # 702 -> 30\n",
    "        self.fc2 = nn.Linear(fc2_num, 1) # 30 -> 1\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 使用xavier的均匀分布对weights进行初始化\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        # 使用正态分布对bias进行初始化\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = self.Inception(data)  # N*702\n",
    "        data = self.fc1(data) # N*30\n",
    "        data = self.relu(data)\n",
    "        data = self.dropout(data)\n",
    "        data = self.fc2(data) # N*1\n",
    "        # 线性激活函数, 无需再进行激活\n",
    "        data = data.to(torch.float)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = AlphaNet(combination, combination_rev, index_list, fc1_num=702, fc2_num=30, dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1             [-1, 1, 36, 3]               2\n",
      "       BatchNorm2d-2             [-1, 1, 36, 3]               2\n",
      "       BatchNorm2d-3              [-1, 1, 9, 3]               2\n",
      "       BatchNorm2d-4              [-1, 1, 9, 3]               2\n",
      "       BatchNorm2d-5              [-1, 1, 9, 3]               2\n",
      "       BatchNorm2d-6              [-1, 1, 9, 3]               2\n",
      "       BatchNorm2d-7              [-1, 1, 9, 3]               2\n",
      "         MaxPool2d-8            [-1, 1, 117, 1]               0\n",
      "       BatchNorm2d-9            [-1, 1, 117, 1]               2\n",
      "        AvgPool2d-10            [-1, 1, 117, 1]               0\n",
      "      BatchNorm2d-11            [-1, 1, 117, 1]               2\n",
      "        MaxPool2d-12            [-1, 1, 117, 1]               0\n",
      "      BatchNorm2d-13            [-1, 1, 117, 1]               2\n",
      "        Inception-14                  [-1, 702]               0\n",
      "           Linear-15                   [-1, 30]          21,090\n",
      "             ReLU-16                   [-1, 30]               0\n",
      "          Dropout-17                   [-1, 30]               0\n",
      "           Linear-18                    [-1, 1]              31\n",
      "================================================================\n",
      "Total params: 21,141\n",
      "Trainable params: 21,141\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(test, input_size=(1, 9, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征维数:  (11825, 9, 30)\n",
      "训练集标签维数:  (11825,)\n",
      "测试集特折维数:  (4943, 9, 30)\n",
      "测试集标签维数:  (4943,)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "X_train = np.load('../data/X_train.npy')\n",
    "y_train = np.load('../data/y_train.npy')\n",
    "X_test = np.load('../data/X_test.npy')\n",
    "y_test = np.load('../data/y_test.npy')\n",
    "# 查看数据的大小\n",
    "print(\"训练集特征维数: \", X_train.shape)\n",
    "print(\"训练集标签维数: \", y_train.shape)\n",
    "print(\"测试集特折维数: \", X_test.shape)\n",
    "print(\"测试集标签维数: \", y_test.shape)\n",
    "\n",
    "# 将数据转换为tensor\n",
    "trainx = torch.from_numpy(np.array(X_train)).reshape(\n",
    "    len(X_train), 1, 9, 30)  # 训练集的特征\n",
    "trainy = torch.from_numpy(np.array(y_train)).reshape(\n",
    "    len(y_train), 1)  # 训练集的标签\n",
    "testx = torch.from_numpy(np.array(X_test)).reshape(\n",
    "    len(X_test), 1, 9, 30)  # 测试集的特征\n",
    "testy = torch.from_numpy(np.array(y_test)).reshape(\n",
    "    len(y_test), 1)  # 测试集的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17      , -0.0031746 , -0.05988024,  0.05206738, -0.03880597,\n",
       "       -0.01670644,  0.11070046,  0.21032767,  0.02749141, -0.00977517])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorData(Dataset):\n",
    "\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.len = len(train_x)\n",
    "        self.x_data = train_x\n",
    "        self.y_data = train_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        指定读取数据的方式: 根据索引index返回dataset[index]\n",
    "\n",
    "        \"\"\"\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 将数据载入到DataLoader中\n",
    "train_data = FactorData(trainx, trainy)\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)  # 不打乱数据集\n",
    "test_data = FactorData(testx, testy)\n",
    "test_loader = DataLoader(dataset=test_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)  # 不打乱数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "alphanet = AlphaNet(combination=combination, combination_rev=combination_rev,\n",
    "                    index_list=index_list, fc1_num=702, fc2_num=30, dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list, bias_list = [], []\n",
    "for name, p in alphanet.named_parameters():\n",
    "    # 将所有的bias参数放入bias_list中\n",
    "    if 'bias' in name:\n",
    "        bias_list += [p]\n",
    "    # 将所有的weight参数放入weight_list中\n",
    "    else:\n",
    "        weight_list += [p]\n",
    "\n",
    "# weight decay: 对所有weight参数进行L2正则化\n",
    "optimizer = optim.RMSprop([{'params': weight_list, 'weight_decay': 1e-5},\n",
    "                           {'params': bias_list, 'weight_decay': 0}],\n",
    "                          lr=1e-4,\n",
    "                          momentum=0.9)\n",
    "# 损失函数为均方误差 MSE\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "epoch_num = 20\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "best_test_epoch, best_test_loss = 0, np.inf\n",
    "seed = 0\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    train_loss, test_loss = 0, 0\n",
    "    # 在训练集中训练模型\n",
    "    alphanet.train()  # 关于.train()的作用，可以参考https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "    train_batch_num = 0\n",
    "    for data, label in tqdm(train_loader, f'Epoch {epoch}-train', leave=False):\n",
    "        train_batch_num += 1\n",
    "        # 准备数据\n",
    "        data, label = data.to(torch.float), label.to(torch.float)\n",
    "        # 得到训练集的预测值\n",
    "        out_put = alphanet(data)\n",
    "        # 计算损失\n",
    "        loss = criterion(out_put, label)\n",
    "        # 将损失值加入到本轮训练的损失中\n",
    "        train_loss += loss.item()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad() # 关于.zero_grad()的作用，可以参考https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        # 反向传播求解梯度\n",
    "        loss.backward()\n",
    "        # 更新权重参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 测试模式\n",
    "    alphanet.eval()\n",
    "    test_batch_num = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(test_loader, f'Epoch {epoch}-test ', leave=False):\n",
    "            test_batch_num += 1\n",
    "            data, label = data.to(torch.float), label.to(torch.float)\n",
    "            # 得到测试集的预测值\n",
    "            y_pred = alphanet(data)\n",
    "            # 计算损失\n",
    "            loss = criterion(y_pred, label)\n",
    "            # 将损失值加入到本轮测试的损失中\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    train_loss_list.append(train_loss/train_batch_num)\n",
    "    test_loss_list.append(test_loss/test_batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出损失函数的变化\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.plot(train_loss_list, 'r', label='train loss')\n",
    "ax.plot(test_loss_list, 'b', label='test loss')\n",
    "# 设置y轴范围\n",
    "ax.set_ylim(bottom=0, top=0.02)\n",
    "ax.legend()\n",
    "ax.autoscale(tight=True)\n",
    "ax.set(xlabel='Batch combination')\n",
    "ax.set(ylabel='Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
