{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成卷积操作时需要的两列数据的组合的列表\n",
    "def generate_combination(N):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        N: int, the number of rows of the matrix\n",
    "\n",
    "    return:\n",
    "        combination: list, the combination of two columns of the matrix\n",
    "        combination_rev: list, the combination of two rows of the matrix, which is the reverse of combination\n",
    "    \"\"\"\n",
    "    col = []\n",
    "    col_rev = []\n",
    "    for i in range(1,N):\n",
    "        for j in range(0,i):\n",
    "            col.append([i,j])\n",
    "            col_rev.append([j,i])\n",
    "    return col, col_rev\n",
    "# 生成卷积操作时需要的两列数据的组合的列表\n",
    "combination, combination_rev = generate_combination(9)\n",
    "\n",
    "# 根据输入的矩阵和卷积操作的步长, 计算卷积操作的索引\n",
    "def get_index_list(matrix, stride):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        matrix: torch.tensor, the input matrix\n",
    "        stride: int, the stride of the convolution operation\n",
    "    \n",
    "    return:\n",
    "        index_list: list, the index of the convolution operation\n",
    "\n",
    "    \"\"\"\n",
    "    W = matrix.shape[3]\n",
    "    if W % stride == 0:\n",
    "        index_list = list(np.arange(0, W+stride, stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        index_list = list(np.arange(0, W+stride-mod, stride)) + [W]\n",
    "    return index_list\n",
    "# 根据输入的矩阵和卷积操作的步长, 计算卷积操作的索引\n",
    "# Inception模块使用的卷积操作的步长为10\n",
    "index_list = get_index_list(np.zeros((1,1,9,30)), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception, 用于提取时间序列的特征, 具体操作包括：\n",
    "\n",
    "    1. kernel_size和stride均为d=10的特征提取层, 类似于卷积层，用于提取时间序列的特征. 具体包括:\n",
    "\n",
    "        1. ts_corr4d: 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的相关系数\n",
    "        2. ts_cov4d: 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的协方差\n",
    "        3. ts_stddev4d: 过去 d 天 X 值构成的时序数列的标准差\n",
    "        4. ts_zscore4d: 过去 d 天 X 值构成的时序数列的平均值除以标准差\n",
    "        5. ts_return4d: (X - delay(X, d))/delay(X, d)-1, 其中delay(X, d)为 X 在 d 天前的取值\n",
    "        6. ts_decaylinear4d: 过去 d 天 X 值构成的时序数列的加权平均值，权数为 d, d – 1, …, 1(权数之和应为 1，需进行归一化处理)，其中离现在越近的日子权数越大\n",
    "        7. ts_mean4d: 过去 d 天 X 值构成的时序数列的平均值\n",
    "\n",
    "        各操作得到的张量维数：\n",
    "        1. 由于涉及两个变量的协方差, 因此ts_corr4d和ts_cov4d的输出为 N*1*36*3\n",
    "        2. 其余操作均只涉及单变量的时序计算, 因此输出为 N*1*9*3\n",
    "\n",
    "    2. 对第1步的输出进行Batch Normalization操作, 输出维数仍为 N*1*36*3 或 N*1*9*3\n",
    "\n",
    "    3. 对于第2步得到的张量, kernel_size为3的池化层. 具体包括:\n",
    "        1. max_pool: 过去 d 天 X 值构成的时序数列的最大值\n",
    "        2. avg_pool: 过去 d 天 X 值构成的时序数列的平均值\n",
    "        3. min_pool: 过去 d 天 X 值构成的时序数列的最小值\n",
    "\n",
    "        以上三个操作的输出均为 N*1*117*1\n",
    "    \n",
    "    4. 对第3步的输出进行Batch Normalization操作, 输出维数仍为 N*1*117*1\n",
    "\n",
    "    5. 将第2步和第4步的输出展平后进行拼接, 得到的张量维数为 N*(2*36*3+5*9*3+3*117) = N*702\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, combination, combination_rev, index_list):\n",
    "        \"\"\"\n",
    "        combination: 卷积操作时需要的两列数据的组合\n",
    "        combination_rev: 卷积操作时需要的两列数据的组合, 与combination相反\n",
    "        index_list: 卷积操作时需要的时间索引\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Inception, self).__init__()\n",
    "        # 卷积操作时需要的两列数据的组合\n",
    "        self.combination = combination\n",
    "        self.combination_rev = combination_rev\n",
    "\n",
    "        # 卷积操作时需要的时间索引\n",
    "        self.index_list = index_list\n",
    "        self.d = len(index_list)-1\n",
    "\n",
    "        # 卷积操作后的Batch Normalization层\n",
    "        self.bc1 = nn.BatchNorm2d(1)\n",
    "        self.bc2 = nn.BatchNorm2d(1)\n",
    "        self.bc3 = nn.BatchNorm2d(1)\n",
    "        self.bc4 = nn.BatchNorm2d(1)\n",
    "        self.bc5 = nn.BatchNorm2d(1)\n",
    "        self.bc6 = nn.BatchNorm2d(1)\n",
    "        self.bc7 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # 池化层, 尺度为1*d\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(1, self.d))\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=(1, self.d))\n",
    "        # 最小池化等价于相反数的最大池化, 后续会对结果取反\n",
    "        self.min_pool = nn.MaxPool2d(kernel_size=(1, self.d))\n",
    "\n",
    "        # 池化操作后的Batch Normalization层\n",
    "        self.bc_pool1 = nn.BatchNorm2d(1)\n",
    "        self.bc_pool2 = nn.BatchNorm2d(1)\n",
    "        self.bc_pool3 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        data: 输入的数据, 维度为batch_size*1*9*30\n",
    "\n",
    "        \"\"\"\n",
    "        # 本层的输入为batch_size*1*9*30, 在训练时不需要反向传播, 因此可以使用detach()函数\n",
    "        data = data.detach().cpu().numpy()\n",
    "        combination = self.combination\n",
    "        combination_rev = self.combination_rev\n",
    "\n",
    "        # 卷积操作\n",
    "        conv1 = self.ts_corr4d(data, combination, combination_rev).to(torch.float)\n",
    "        conv2 = self.ts_cov4d(data, combination, combination_rev).to(torch.float)\n",
    "        conv3 = self.ts_stddev4d(data).to(torch.float)\n",
    "        conv4 = self.ts_zcore4d(data).to(torch.float)\n",
    "        conv5 = self.ts_return4d(data).to(torch.float)\n",
    "        conv6 = self.ts_decaylinear4d(data).to(torch.float)\n",
    "        conv7 = self.ts_mean4d(data).to(torch.float)\n",
    "\n",
    "        # 卷积操作后的Batch Normalization\n",
    "        batch1 = self.bc1(conv1)\n",
    "        batch2 = self.bc2(conv2)\n",
    "        batch3 = self.bc3(conv3)\n",
    "        batch4 = self.bc4(conv4)\n",
    "        batch5 = self.bc5(conv5)\n",
    "        batch6 = self.bc6(conv6)\n",
    "        batch7 = self.bc7(conv7)\n",
    "\n",
    "        # 在 H 维度上进行特征拼接\n",
    "        feature = torch.cat(\n",
    "            [batch1, batch2, batch3, batch4, batch5, batch6, batch7], axis=2)  # N*1*(2*36+5*9)*3 = N*1*117*3\n",
    "\n",
    "        # 同时将特征展平, 准备输入到全连接层\n",
    "        feature_flatten = feature.flatten(start_dim=1)  # N*(117*3) = N*351\n",
    "\n",
    "        # 对多通道特征进行池化操作, 每层池化后面都有Batch Normalization\n",
    "        # 最大池化\n",
    "        maxpool = self.max_pool(feature)  # N*1*117*1\n",
    "        maxpool = self.bc_pool1(maxpool)\n",
    "        # 平均池化\n",
    "        avgpool = self.avg_pool(feature)  # N*1*117*1\n",
    "        avgpool = self.bc_pool2(avgpool)\n",
    "        # 最小池化\n",
    "        # N*1*117*1, 最小池化等价于相反数的最大池化, 并对结果取反\n",
    "        minpool = -self.min_pool(-1*feature)\n",
    "        minpool = self.bc_pool3(minpool)\n",
    "        # 特征拼接\n",
    "        pool_cat = torch.cat([maxpool, avgpool, minpool],\n",
    "                             axis=2)  # N*1*(3*117)*1 = N*1*351*1\n",
    "        # 将池化层的特征展平\n",
    "        pool_cat_flatten = pool_cat.flatten(start_dim=1)  # N*351\n",
    "\n",
    "        # 拼接展平后的特征\n",
    "        feature_final = torch.cat(\n",
    "            [feature_flatten, pool_cat_flatten], axis=1)  # N*(351+351) = N*702\n",
    "        return feature_final\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的相关系数\n",
    "    def ts_corr4d(self, Matrix, combination, combination_rev):\n",
    "        new_H = len(combination)\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的相关系数\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, combination, start_index:end_index]  # N*1*new_H*2*d\n",
    "            data2 = Matrix[:, :, combination_rev,\n",
    "                           start_index:end_index]  # N*1*new_H*2*d\n",
    "            std1 = data.std(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求标准差\n",
    "            std2 = data2.std(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求标准差\n",
    "            std = (std1*std2).mean(axis=3, keepdims=True)  # N*1*new_H*1*1\n",
    "            list.append(std)\n",
    "        std = np.squeeze(np.array(list)).transpose(1, 2, 0).reshape(-1, 1, new_H,\n",
    "                                                                    len(index_list)-1)+0.01  # N*1*new_H*len(index_list)-1 # 加上0.01, 防止除0\n",
    "        # N*1*new_H*len(index_list)-1\n",
    "        cov = self.ts_cov4d(Matrix, combination, combination_rev)\n",
    "        corr = cov/std  # N*1*new_H*len(index_list)-1\n",
    "        return corr\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列和 Y 值构成的时序数列的协方差\n",
    "    def ts_cov4d(self, Matrix, combination, combination_rev):\n",
    "        new_H = len(combination)\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的协方差\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, combination, start_index:end_index]  # N*1*new_H*2*d\n",
    "            data2 = Matrix[:, :, combination_rev,\n",
    "                           start_index:end_index]  # N*1*new_H*2*d\n",
    "            mean1 = data.mean(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求均值\n",
    "            mean2 = data2.mean(axis=4, keepdims=True)  # N*1*new_H*2*1, 在时序上求均值\n",
    "            spread1 = data - mean1  # N*1*new_H*2*d, 在时序上求偏差\n",
    "            spread2 = data2 - mean2  # N*1*new_H*2*d, 在时序上求偏差\n",
    "            cov = ((spread1 * spread2).sum(axis=4, keepdims=True) /\n",
    "                   (data.shape[4]-1)).mean(axis=3, keepdims=True)  # N*1*new_H*1*1\n",
    "            list.append(cov)\n",
    "        cov = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(cov)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的标准差\n",
    "    def ts_stddev4d(self, Matrix):\n",
    "        # 只需要对单变量做卷积操作, 不需要将变量两两组合。因此输出的 H 可以保持和输入的 H 一致\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的标准差\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            std = data.std(axis=3, keepdims=True)  # N*1*H*1\n",
    "            list.append(std)\n",
    "        std4d = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(std4d)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的平均值除以标准差\n",
    "    def ts_zcore4d(self, Matrix):\n",
    "        # 只需要对单变量做卷积操作, 不需要将变量两两组合。因此输出的 H 可以保持和输入的 H 一致\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的zcore\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            mean = data.mean(axis=3, keepdims=True)  # N*1*H*1\n",
    "            std = data.std(axis=3, keepdims=True) + \\\n",
    "                0.01  # N*1*H*1, 加上0.01, 防止除以0\n",
    "            list.append(mean/std)\n",
    "        zscore = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(zscore)\n",
    "\n",
    "    # (X - delay(X, d))/delay(X, d)-1, 其中 delay(X, d)为 X 在 d 天前的取值\n",
    "    def ts_return4d(self, Matrix):\n",
    "        # 只需要对单变量做卷积操作, 不需要将变量两两组合。因此输出的 H 可以保持和输入的 H 一致\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的return\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            # N*1*H*1, 在分母加上0.01, 防止除以0\n",
    "            return_ = data[:, :, :, -1]/(data[:, :, :, 0]+0.01)-1\n",
    "            list.append(return_)\n",
    "        ts_return = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(ts_return)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的加权平均值, 权数为 d, d – 1, …, 1(权数之和应为 1, 需进行归一化处理), 其中离现在越近的日子权数越大\n",
    "    def ts_decaylinear4d(self, Matrix):\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的加权平均值\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            range_ = end_index-start_index\n",
    "            weight = np.arange(1, range_+1)\n",
    "            weight = weight/weight.sum()  # 权重向量\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            wd = (data*weight).sum(axis=3, keepdims=True)  # N*1*H*1\n",
    "            list.append(wd)\n",
    "        ts_decaylinear = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(ts_decaylinear)\n",
    "\n",
    "    # 过去 d 天 X 值构成的时序数列的平均值\n",
    "    def ts_mean4d(self, Matrix):\n",
    "        new_H = Matrix.shape[2]\n",
    "        index_list = self.index_list\n",
    "        list = []  # 存放长度为len(index_list)-1的平均值\n",
    "        for i in range(len(index_list)-1):\n",
    "            start_index = index_list[i]\n",
    "            end_index = index_list[i+1]\n",
    "            data = Matrix[:, :, :, start_index:end_index]  # N*1*H*d\n",
    "            mean_ = data.mean(axis=3, keepdims=True)  # N*1*H*1\n",
    "            list.append(mean_)\n",
    "        ts_mean = np.squeeze(np.array(list)).transpose(\n",
    "            1, 2, 0).reshape(-1, 1, new_H, len(index_list)-1)  # N*1*new_H*len(index_list)-1\n",
    "        return torch.from_numpy(ts_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaNet(nn.Module):\n",
    "\n",
    "    def __init__(self, combination, combination_rev, index_list, fc1_num, fc2_num, dropout_rate):\n",
    "        super(AlphaNet, self).__init__()\n",
    "        self.combination = combination\n",
    "        self.combination_rev = combination_rev\n",
    "        self.fc1_num = fc1_num\n",
    "        self.fc2_num = fc2_num\n",
    "        # 自定义的Inception模块\n",
    "        self.Inception = Inception(combination, combination_rev, index_list)\n",
    "        # 两个全连接层\n",
    "        self.fc1 = nn.Linear(fc1_num, fc2_num) # 702 -> 30\n",
    "        self.fc2 = nn.Linear(fc2_num, 1) # 30 -> 1\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 使用xavier的均匀分布对weights进行初始化\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        # 使用正态分布对bias进行初始化\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = self.Inception(data)  # N*702\n",
    "        data = self.fc1(data) # N*30\n",
    "        data = self.relu(data)\n",
    "        data = self.dropout(data)\n",
    "        data = self.fc2(data) # N*1\n",
    "        # 线性激活函数, 无需再进行激活\n",
    "        data = data.to(torch.float)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = AlphaNet(combination, combination_rev, index_list, fc1_num=702, fc2_num=30, dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(test, input_size=(1, 9, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "X_train = np.load('../data/X_train.npy')\n",
    "y_train = np.load('../data/y_train.npy')\n",
    "X_test = np.load('../data/X_test.npy')\n",
    "y_test = np.load('../data/y_test.npy')\n",
    "# 查看数据的大小\n",
    "print(\"训练集特征维数: \", X_train.shape)\n",
    "print(\"训练集标签维数: \", y_train.shape)\n",
    "print(\"测试集特折维数: \", X_test.shape)\n",
    "print(\"测试集标签维数: \", y_test.shape)\n",
    "\n",
    "# 将数据转换为tensor\n",
    "trainx = torch.from_numpy(np.array(X_train)).reshape(\n",
    "    len(X_train), 1, 9, 30)  # 训练集的特征\n",
    "trainy = torch.from_numpy(np.array(y_train)).reshape(\n",
    "    len(y_train), 1)  # 训练集的标签\n",
    "testx = torch.from_numpy(np.array(X_test)).reshape(\n",
    "    len(X_test), 1, 9, 30)  # 测试集的特征\n",
    "testy = torch.from_numpy(np.array(y_test)).reshape(\n",
    "    len(y_test), 1)  # 测试集的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorData(Dataset):\n",
    "\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.len = len(train_x)\n",
    "        self.x_data = train_x\n",
    "        self.y_data = train_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        指定读取数据的方式: 根据索引index返回dataset[index]\n",
    "\n",
    "        \"\"\"\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 将数据载入到DataLoader中\n",
    "train_data = FactorData(trainx, trainy)\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)  # 不打乱数据集\n",
    "test_data = FactorData(testx, testy)\n",
    "test_loader = DataLoader(dataset=test_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)  # 不打乱数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "alphanet = AlphaNet(combination=combination, combination_rev=combination_rev, index_list=index_list, fc1_num=702, fc2_num=30, dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight decay: 对所有weight参数进行L2正则化\n",
    "weight_list,bias_list = [],[]\n",
    "for name,p in alphanet.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        bias_list += [p]\n",
    "    else:\n",
    "        weight_list += [p]\n",
    "optimizer = optim.SGD([{'params': weight_list, 'weight_decay':1e-5},\n",
    "                       {'params': bias_list, 'weight_decay':0}],\n",
    "                      lr = 1e-3,\n",
    "                      momentum = 0.9)\n",
    "criterion = nn.MSELoss()\n",
    "#training\n",
    "epoch_num = 1\n",
    "loss_list = []\n",
    "test_loss = []\n",
    "for epoch in range(epoch_num ):\n",
    "    for data,label in train_loader:\n",
    "        # break\n",
    "        #训练模式\n",
    "        alphanet.train()#training pattern, grad required.\n",
    "        out_put = alphanet(data.to(torch.float))\n",
    "        loss = criterion(out_put,label.to(torch.float))\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() #renew the parameters\n",
    "        \n",
    "        #测试模式\n",
    "        alphanet.eval()\n",
    "        y_pred = alphanet(testx.to(torch.float))\n",
    "        testloss = criterion(y_pred,testy)\n",
    "        test_loss.append(testloss.item())\n",
    "        #print(\"ok\")\n",
    "    \n",
    "    #print(\"current epoch time:\",epoch+1)\n",
    "    #print(\"current loss of epoch \",epoch+1,\":\", train_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "with plt.style.context(['ggplot']):\n",
    "    fig, ax = plt.subplots(figsize = (10,4))\n",
    "    x = list(np.arange(1,len(test_loss)+1))\n",
    "    y = test_loss \n",
    "    ax.plot(x,y)\n",
    "    ax.legend(title = \"Loss on Training Set\")\n",
    "    ax.autoscale(tight = True)\n",
    "    ax.set(xlabel = 'Batch combination')\n",
    "    ax.set(ylabel = 'Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
